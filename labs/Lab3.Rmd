---
title: "Lab3"
output: html_document
---

## Part 1A: Prior Distribution

Plot the prior distribution for p(head) using a truncated exponential:
`dexp(x, rate=5) / 0.9932621` for x in [0,1], 0 otherwise.

```{r part1a}
x_seq <- seq(0, 1, length.out = 1000)

prior_exp <- function(x) {
  ifelse(x >= 0 & x <= 1, dexp(x, rate = 5) / 0.9932621, 0)
}

plot(x_seq, prior_exp(x_seq),
     type = "l", col = "blue", lwd = 2,
     main = "Prior Distribution: Truncated Exponential (rate=5)",
     xlab = "p(head)", ylab = "Density")
```

---

## Part 1B: Posterior with 14 Heads and 10 Tails

Three approaches:
1. Metropolis algorithm with exponential prior
2. Grid approximation with exponential prior
3. Exact analytical solution from Beta(40,40) prior

```{r part1b}
set.seed(42)

heads_1b <- 14
tails_1b <- 10

# ── Metropolis Algorithm ──────────────────────────────────────────────────────

likelihood <- function(p, h, t) {
  dbinom(h, h + t, p)
}

metropolis <- function(n_steps, h, t, prior_fn, step_size = 0.05) {
  chain <- numeric(n_steps)
  current <- 0.5

  for (i in seq_len(n_steps)) {
    proposal <- current + runif(1, -step_size, step_size)

    if (proposal < 0 || proposal > 1) {
      chain[i] <- current
      next
    }

    accept_ratio <- (likelihood(proposal, h, t) * prior_fn(proposal)) /
                    (likelihood(current,  h, t) * prior_fn(current))

    if (runif(1) < accept_ratio) {
      current <- proposal
    }
    chain[i] <- current
  }
  chain
}

metro_samples_1b <- metropolis(n_steps   = 100000,
                               h         = heads_1b,
                               t         = tails_1b,
                               prior_fn  = prior_exp,
                               step_size = 0.05)

# Burn-in: discard first 10%
metro_samples_1b <- metro_samples_1b[-(1:10000)]

# ── Build common x-axis from Metropolis histogram ────────────────────────────

myHist <- hist(metro_samples_1b, breaks = 200, plot = FALSE)

x_mids <- myHist$mids

# ── Grid Approximation ───────────────────────────────────────────────────────

grid_posterior <- function(x, h, t, prior_fn) {
  vals <- likelihood(x, h, t) * prior_fn(x)
  vals / sum(vals)
}

grid_post_1b <- grid_posterior(x_mids, heads_1b, tails_1b, prior_exp)

# ── Exact Analytical: Beta(40,40) prior → Beta(40+14, 40+10) posterior ───────

beta_prior_alpha <- 40
beta_prior_beta  <- 40

beta_post_alpha <- beta_prior_alpha + heads_1b   # 54
beta_post_beta  <- beta_prior_beta  + tails_1b   # 50

beta_post_fn <- function(x) dbeta(x, beta_post_alpha, beta_post_beta)
beta_post_vals <- beta_post_fn(x_mids)
beta_post_norm <- beta_post_vals / sum(beta_post_vals)

# ── Plot ─────────────────────────────────────────────────────────────────────

# Metropolis (normalized counts)
metro_norm <- myHist$counts / length(metro_samples_1b)

plot(x_mids, metro_norm,
     type = "l", col = "black", lwd = 2,
     main = "Posterior: 14 Heads, 10 Tails",
     xlab = "p(head)", ylab = "Normalized Density",
     ylim = c(0, max(metro_norm, grid_post_1b, beta_post_norm) * 1.1))

lines(x_mids, grid_post_1b,    col = "red",  lwd = 2, lty = 2)
lines(x_mids, beta_post_norm,  col = "blue", lwd = 2, lty = 3)

legend("topright",
       legend = c("Metropolis (exp prior)",
                  "Grid approx (exp prior)",
                  "Exact: Beta(40,40) prior"),
       col    = c("black", "red", "blue"),
       lwd    = 2,
       lty    = c(1, 2, 3))
```

The Metropolis and grid approximation curves (both using the exponential prior)
closely agree, confirming the algorithm is working. The Beta(40,40) posterior is
narrower and centered near 0.5, because the Beta(40,40) prior is strongly
informative around 0.5, while the exponential prior favours values near 0,
pulling that posterior to the left.

---

## Part 1C: Posterior with 583 Heads and 417 Tails

```{r part1c}
set.seed(42)

heads_1c <- 583
tails_1c <- 417

# ── Metropolis ───────────────────────────────────────────────────────────────
# With many observations the likelihood is sharply peaked; a smaller step size
# keeps the acceptance rate healthy.

metro_samples_1c <- metropolis(n_steps   = 200000,
                               h         = heads_1c,
                               t         = tails_1c,
                               prior_fn  = prior_exp,
                               step_size = 0.01)

metro_samples_1c <- metro_samples_1c[-(1:20000)]   # burn-in

myHist_1c <- hist(metro_samples_1c, breaks = 200, plot = FALSE)

x_mids_1c <- myHist_1c$mids

# ── Grid Approximation ───────────────────────────────────────────────────────

grid_post_1c <- grid_posterior(x_mids_1c, heads_1c, tails_1c, prior_exp)

# ── Exact Analytical: Beta(40+583, 40+417) ───────────────────────────────────

beta_post_alpha_1c <- beta_prior_alpha + heads_1c   # 623
beta_post_beta_1c  <- beta_prior_beta  + tails_1c   # 457

beta_post_fn_1c  <- function(x) dbeta(x, beta_post_alpha_1c, beta_post_beta_1c)
beta_post_1c     <- beta_post_fn_1c(x_mids_1c)
beta_post_norm_1c <- beta_post_1c / sum(beta_post_1c)

# ── Plot ─────────────────────────────────────────────────────────────────────

metro_norm_1c <- myHist_1c$counts / length(metro_samples_1c)

plot(x_mids_1c, metro_norm_1c,
     type = "l", col = "black", lwd = 2,
     main = "Posterior: 583 Heads, 417 Tails",
     xlab = "p(head)", ylab = "Normalized Density",
     ylim = c(0, max(metro_norm_1c, grid_post_1c, beta_post_norm_1c) * 1.1))

lines(x_mids_1c, grid_post_1c,      col = "red",  lwd = 2, lty = 2)
lines(x_mids_1c, beta_post_norm_1c, col = "blue", lwd = 2, lty = 3)

legend("topright",
       legend = c("Metropolis (exp prior)",
                  "Grid approx (exp prior)",
                  "Exact: Beta(40,40) prior"),
       col    = c("black", "red", "blue"),
       lwd    = 2,
       lty    = c(1, 2, 3))
```

### Discussion

With 583 heads and 417 tails the three posterior curves are **much more tightly
clustered** and nearly indistinguishable from one another, compared with the
14H/10T case.

**Why does this plot look different from 1B?**

* **More data swamps the prior.** With ~1000 coin flips the likelihood function
  is extremely concentrated around the observed proportion (583/1000 = 0.583).
  That concentration dominates any reasonable prior — whether exponential or
  Beta(40,40). Because the likelihood overwhelms the prior, the choice of prior
  matters very little, and all three posteriors converge to essentially the same
  narrow peak near p ≈ 0.58.

* **In 1B** we had only 24 flips. The prior still carried significant weight,
  so the exponential prior (which favours small p) pulled the exp-prior
  posteriors noticeably to the left relative to the Beta(40,40) posterior
  (which is centred at 0.5).

* **This is the Bayesian large-sample consistency property**: as N → ∞,
  the posterior concentrates at the true parameter value regardless of the
  (reasonable) prior chosen.
